┏━┳━┓┃┣╋┫┗┻┛
06 웹 크롤링으로 정보 모으기

여러분이 프로그래밍 관련 일을 하지 않는다면, 웹 크롤링만큼 여러분의 실생활에 직접적인 도움을 줄 수 있는 기술도 드뭅니다. 이 장에서는 어떻게 실생활에 웹 크롤링을 활용하는지 알아보겠습니다.

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃	06-1 웹 크롤링 알아보기				┃
┃	06-2 웹 크롤링 준비하기				┃
┃	06-3 포털 사이트에서 기사 크롤룅 하기	┃
┃	06-4 프로그램 실행 파일 만들기			┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

06-1 웹 크롤링 알아보기

웹은 끊입없이 커지는 도서관과 같습니다. 새로운 정보가 끊임없이 생겨나죠. 구글 같은 회사가 하는 일 중 하나가 새로 생기는 정보를 구글 검색에 바로 '걸리게'하는 것입니다. 이때 전 세계에서 끊임없이 생산되는 새로운 정보를 모으기 위해 필요한 일이 바로 웹 크롤링(Webcrawling)입니다.

웹 크롤링이란
웹 크롤링이란 웹의 정보를 자동으로 수집하는 것을 의미하며 ㅜ이런 목적을 위해 만든 프로그램을 웹 크롤러라고 말합니다. 섬색 엔진의 아주 원시적인 형태라고 할 수 잇죠. 다른 한 편으로 어두운 해커들의 세계와 연결되어 있기도 합니다. 웹의 정보를 모으는데 웹 크롤링은 아주 매력적인 접근 방법이지만, 한편으로 위험하기도 합니다.

이 책에서는 웹 크롤링의 기본적인 내용만 다룹니다. 제가 웹 크롤링을 배울 때 다른 사람들의 웹 사이트에 있는 스크립트를 그대로 실행해 보면 환경이 바뀐 탓에 제대로 작동하지 않을 때가 많았습니다. 여기서 나온 코드 역시 시간이 지나면 작동하지 않을 수 있습니다. 따라서 제 깃허브(https://github.com/skytreesea/do-it-python)에 실습 코드를 꾸준히 업데이트해 언제든지 작동되는 코드를 사용할 수 있도록 해 두겠습니다.
%해당 코드를 실무에 그대로 사용하지 마십시오. 저자와 출판사는 실무 사용에 따르는 법적인 책임을 지지 않습니다.

HTML 몰라도 웹 크롤링을 할 수 있을까
매일 주요 뉴스를 모아서 텍스트 파일로 저장하고 싶다면 어떤 기술이 필요할까요? 요즘은 블로그 포스팅 등에서도 웹 크롤링을 많이 소개하고 있어 아미 크롤링을 접한 사람도 있을 것입니다. 크롤링을 배우고 싶어서 검색으로 정보를 얻어 보지만, 잠깐 검색한 내용만 가지고 크롤링에 도전해 보기는 쉽지 않죠.

웹 크롤링이라느 ㄴ과정을 온전히 이해가기 위해서는 HTML(하이퍼텍스트 마크업 언어, Hyper Text Markup Language)에 대한 지식이 어느 정도 있어야 합니다. 왜냐하면 웹은 기본적으로 HTML로 만들어져 있기 떄문입니다. 그런데 HTML을 읽을 줄 모른다고요? 걱장하지 마세요. 컴퓨터를 전공하지 않거나, 프로그래밍을 배우지 않은 사람도 크롤링을 할 수 있게 하는 것이 이 장의 목적입니다. 저 역시 HTML이나 CSS의 모든 것을 알고 있지 않습니다. 웹 크롤링이 목적이라면 전부를 알 필요는 없기 떄문입니다.

인터넷에는 중요한 자료가 정말 없을까
'인터넷에는 중요한 자료가 없다'는 말이 있죠? 이제 새삼 반박할 필요도 없겠지만, 이 세상에 거의 모든 정보는 인터넷에 있다고 해도 과언이 아닙니다. 심지어는 저작권이 없어진 고전들 역시 구텐베르크 사이트에서 무료로 볼 수 있습니다. 그뿐만 아니라, 조선왕조실록 등 사료들 역시 인터넷 사이트에 체계적으로 정리가 잘 되어 있습니다. 몰론 이것을 가공해 활요하는 것은 사람의 몫이겠지만, 이제 인터넷에 중요한 자료가 없다고 말하기는 힘들곘죠?

최신 학문적 논의 역시 구글 스칼라나 위키피디아에 잘 정리되어 있습니다. 많은 사람이 위키피디아에 '전문적 자료가 없다'고 말하기도 합니다만, 위키피디아만큼 어떤 주제에 대해서 핵심적인 내용을 체계적으로 정리해 놓은 사이트는 드뭅니다. 박사학위를 마친 연구자가 감사의 글에 '고마워요, 위키(Thank you. Wikipedia)'라고 썼다는 우스갯소리가 나올 정도이지요. 이것도 벌써 엄청나게 오랜된 유머입니다.

수많은 정보를 어떻게 활용할까
크롤링은 필요한 자료를 빠른 시간에 수집할 수 있는 엄청나게 강력한 도구입니다. 때로는 웹사이트로부터 너무나 쉽게 정보를 획득할 수 있어서 약간은 위험하다고 느낄 만큼 엄청난 정보를 얻어낼 수 있습니다.

아주 간단한 예로 지금 당장 코스피 상위 200개 종목의 주가를 한 번에 확인해서 그래프로 출력하고 싶을 때, 웹 크롤링으로 필요한 자료를 쉽손게 가져올 수 있습니다. 몰론 이런 기능은 증권사 HTS에서 너무나 잘 구현되어 있습니다만, 자기만의 자료 수집 방식을 가진다는 것은 엄청나게 매력적인 일입니다.

앞으로 소개할 웹 클롤링 관룐 코드는 연구 직종에 종사하고 있는 제가 개인적인 필요를 위해서 제작했던 것입니다. 저는 이런 웹 크롤링의 도움을 많이 받았으며, 이것을 잘 활용하면 다른 사람들이 따라오기 힘들 정도의 생산력을 발휘할 수 있습다고 믿습니다.

%알아보면 좋아요 - 데이터 수집 시 주의할 점
본격적으로 시작하기 전에 먼저 주의할 사항이 있습니다. 먼저 클롤링의 세계는 인터넷의 크기만큼이나 무한하지만, 이 책에서 언급하는 크롤링의 범위는 매우 제한적이라는 점입니다. 크롤링의 세계는 결국 해킹의 세계와도 연결됩니다. 처음에는 크롤링이나 스크래핑(scraping)에 만족하다가 나중에는 암호화된 정보에 접근하고 싶은 욕망이 생길지도 모릅니다. 이 책에서는 크롤링 중에서도 아주 파편적이며 매우 제한적인 부분만 다룹니다. 다시 말해 신문 기사를 모을 수 있는 정도의 크롤링 기법만을 설명합니다. 그런데 이러한 크롤링이 가능하다는 사실에 알게 되고, 또 실습을 하다 보면, 데이터를 보는 눈이 달라집니다. 같은 문제로 정보를 검색하더라도 어떤 부분은 자동화할 수 있겠다는 감을 잡게 해주는 것이 이 장의 목적 입니다.

또 하나의 유의할 점은 크롤링의 접근 방식은 법적 문제를 일으킬 수 있다는 점입니다. '크롤링 남의 자산 훔치는 범죄행위, 인식 변화 갖자'라는 무시무시한 제목의 시가(IT 조선, 박철현 기자, 2019.05.07.)가 있을 정도입니다. 그런데 해당 기사에서 언급하고 있듯이, 크롤링 대부분은 위법이 아닙니다. '하지만 웹페이지의 운영자가 긁어가지 못하도록 조치한 데이터를 긁어간다거나, 긁어간 데이터를 사용해 부당이득을 얻는다거나 하는 행위를 할 경우엔 저작권법이나 부정경쟁방지법 등의 재재를 받을 수 있다.'고 이기사는 설명하고 있습니다.

이 책에서 공개하는 크롤링 방식은 지극히 개인적이고 파편적인 수준이며, 상업적 가치가 있는 데이터에 접근을 시도할 때 발생할 수 있는 어떠한 상황에 대래서는 책임을 질 수 없습니다. 하지만 크롤링 자체는 검색 서비스를 비롯한 전 세계의 수많은 프로그래머가 웹의 저옵를 모으는 아주 일반적인 방식이라는 점은 짚고 넘어가겠습니다.

06-2 웹 크롤링 준빈하기

여러분, 명언 좋아하나요? 저는 유염인의 어록을 종종 찾아 읽습니다. 그 사람의 인생과 철학이 느껴지거든요. 예를 들어 '내가 조금 더 보았다면, 그건 거인의 어깨 위에 서 있기 때문이다.(if i have seen further it is by standing on the shoulders of Giants.).'라는 뉴턴의 명언을 읽으면 그가 얼마나 겸손했는지를 느낄 수 있지요. 게다가 영어 공부는 덤으로 되고요. 그럼 이런 명언이 모여 있는 웹 사이트에서 크롤링을 통해 명언을 수집해 보겠습니다.

이런 생활이라면?
웹 사이트에서 명언을 수집하자
영어 명언 수집하는 것을 좋아하는 저에게 한 친구가 명언을 모아 놓은 웹 사이틀를 추천해 주었습니다. 접속해 보니 그야말로 명언 천국이네요. 아인슈타인의 명언부터 해리포터 시리즈의 점블도어 교장 선생님의 대사까지 '명언'으로 꽉 차 있네요. 그런데 일일이 옮겨 적으려니 시간이 너무 많이 들 것 같습니다. 문서 파일에 한 번에 모아서 저장할 수 없을까요?

'quotes to scrape'라는 사이틀 소개합니다.(https://quotes.toscrape.com/). 이 사이트는 말 그대로 스크랩(scrap)하기 좋은 명언을 정리해 놓은 사이트입니다. 이 사이트의 명언을 엑셀 파일로 만드어서 관리하고 싶은데요.

그러기 위해서 명언을 하나씩 복사해 옭기는 방법이 있을 겁니다. 아직도 코드 자기가 귀찮아서 필요한 내용을 종종 이렇게 마우스로 긁어서 가져올 때가 있습니다. 하지만 양이 늘어나면 그 작업 자체가 쉽지 않을뿐더러, 마우스가 제대로 움직이지 않아서 힘이 들곤 합니다.

반복은 컴퓨터가 훨씬 잘하다고 했지요? 바로 이럴 때 웹 크롤러를 만들면 명언을 쉽게 가져올 수 있습니다. 이 사이트를 활요해 웹 크롤링을 간단히 실습해 보죠.

Quotes to scrape 페이지 살펴보기
짜, 머넞 사이트를 그냥 한번 들여다보겠습니다. 먼저 사이트가 어떻게 생겼는지 알아야 사이트에서 어떻게 저옵를 얻어낼지 생각할 수 있습니다. 명언을 살펴보니 문장마다 밑에 태그 가 붙어있고, 오른쪽에는 가장 많이 쓰인 태그가 나열되어 있습니다. 여기서 <life>를 클릭해 보겠습니다.

다음과 같이 'life'라는 태크로 된 화면으로 넘어갑ㄴ디ㅏ. '세상에는 두 종류의 삶이 있다. 하나 기적이란 하나도 없는 인생이다. 다른 하나는 모든 것이 기적인 삶이다.'라는 아인슈타인의 명언이 있네요. 여기서 <(about)>을 클릭해 볼까요?

해당 인물의 정보가 나오네요. 인물 정보도 따로 모아볼 수 있겠네요.

이전 페이지로 돌아가 스크롤을 내려 보면 <Next> 버튼이 있습니다. 클릭해 볼까요?

클릭하면 다음 화면으로 넘어가면서 새로운 명언들이 나오는 것을 볼 수 있스빈다. 밑에 있는 <previous> 번튼을 누르면 다시 이전 페이지로 돌아갑니다.

지금까지 Quotes to scrape 사이트가 어떻게 생겼는지 간단히 살펴보았습니다. 지그은 웹사이트에 접속해서 정보를 얻어내는 것이 일상화되어 있어 웹 사이트의 특성을 알아내는 일은 식은 죽 먹기라 생각했을지도  모르겠습니다. 그런데 웹 사이트 둘러보기로 두 가지 사실을 알았습니다.

먼저, 이 사이트의 명언들은 태그별로 모여 있다는 사실입니다. 인생(life)과 사랑(love)에 대한 명언을 태그별로 모으는 것도 가능합니다. 또한, 한 페이지에 일정 수의 명언만 보이고 나너지는 다음(Next) 페이지에 저장되어 있다는 사실을 확인했습니다.

자, 그렇다면 앞으로 해야 할 일은 다음과 같이 정의할 수 있겠습니다. 먼저 웹 사이트에 접속해서 한 태그의 명어늘 긁어옵니다. 그리고 다음(Next) 페이지로 이동해 명언을 계속 긁어옵니다. 다음 페이지가 ㅇㅄ다면 다른 태그로 넘어갑니다. 이 과정은 복잡하고 어려운 작업까지 이어질 수 있지만, 차근차근히 진행해 보겠습니다.

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃	1. 'Quotes to scrape'페이지를 불러와서 객체에 저장한다.	┃
┃	2. 브라우저ㅗ에서 명언이 았는 위치를 확인한다.             	┃
┃	3. 각 태그에 맞는 명언을 찾아서 출력한다.                	┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

뷰티풀수프 설치하기
파이써능로 웹 크롤링을 하기 위해 먼저 뷰티풀수프(BeautifulSoup)를 설치하겠습니다. 뷰티풀수프는 웹 문서를 구성하는 HTML과 XML 문서에서 원하는 정보를 쉽게 추출할 수 있는 모듈을 모아 놓은 파이썬 패키지입니다. 아나콘다에 포함되어씨지 않아 별도로 설치를 꼭 햐야 합니다.
%셀레니움(selenium)의 웹드라이버(webdriveer)로 브라우저를 제어하는 방법도 있지만 여기서는 뷰티풀수프 패키지를 이용한 스크래핑(scraping)만 알아보겠습니다.

pip를 사용하면 패키지를 쉽게 설치할 수 있습니다. 명령 프롬프트 창을 열고 다음 명령을 입력해 뷰티풀수프 패키지를 설치하세요.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃	C:\User\user>pip install beatifulsoup4	┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

기분 모듈 임포트 하기
본격적으로 코드를 작성하기 전에 필요한 것 같은 모듈을 미리 불러오겠습니다. 웹 크롤링 실습에는 어떤 모듈이 필요할까요? 먼저 지금까지 항상 써 왔던 운영체제 모듈 os, 정규식 모듈 re, 그리고 앞에서 만들었던 CSV 파일을 저장할 때 필요한 usercsv 모듈을 임포트 하겠습니다.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃	>>> import os, re, usercsv	┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

이제 웹 크롤링을 하기 위해 새로 알아야 할 모듈을 살펴보겠습니다. 먼저 requests는 URL주소에 있는 내용을 요청할 때 사용하는 모듈입니다. os, re와 마찬가지로 파이썬에 내장되어 있어 바로 임포트 하면 됩니다.
┏━━━━━━━━━━━━━━━━━━━━━━━┓
┃	>>> import requests	┃
┗━━━━━━━━━━━━━━━━━━━━━━━┛

다음으로 urllib, request 모듈을 임포트 하겠습니다. urllib은 웹에서 얻은 데이터를 다루는 파이썬 패키지입니다. 이 패키지 안에는 총 네 개의 모듈이 있는데, 이중 웹 문서를 열어 데이터를 읽어오는 request 모듈을 사용하겠습니다.

편하게 사용할 수 있도록 urllib.request 모듈을 ur로 임포트 합니다. 줄임말은 사용하기 편한 말로 여러분 마음대로 자유롭게 작성해서 됩니다.

	>>> import urllib.request as ur		# ur로 줄여서 임포트 압니다.
	
마지막으로 앞에서 설치한 뷰티풀수프 패키지에서 BeautifulSoup를 bs로 임포트 합니다.

	>>> from bs4 import BeautifulSoup as bs 	#bs로 줄여서 임포트 합니다.
	
사실 ur, bs 모듈 사용법이 이 장에서 다룰 내용의 전부라고 해도 과언이 아닙니다. 웹의 정보를 수집할 수 있다면, 여러분은 엄청나게 강력한 무기를 가지게 되는 셉입니다. 전 세계에서 엄청나게 많은 살람이 지금, 이 순간에도 무언가 일을 해서 그 자료를 웹에 올려놓고 있기 때문이죠.

Do it 웹 문서 자료를 가져와 가공하기
이제부터 웹 문서 자료를 파이썬으로 가져와 추출하기 좋게 그 형태를 가공해 보겟습니다.

1. urlopen 으로 웹 사이트 정보 가져오기
먼저 접속하고 싶은 웹 사이트의 URL 주소를 url이라는 객체에 저장해 줍니다. 여기서 url 객체에 저장되는 건 그냥 문자열이며 아무런 의미도 없습니다.

	>>> url = 'http://quotes.toscrape.com/'

즉, 현재 상황은 url 객체 안에 'http://quotes.toscrape.com/'라는 문자열이 들어 있을 뿐입니다. 이 주소에 해당하는 웹 사이트에 원하는 정보를 요청해서 그 결과물을 반환하는 명령은 다음과 같습니다.

	urllib.request.urlopen('URL 주소')
	
urllib.request를 ur로 줄여서 임포트 했으므로 ur.urlopen('URL 주소')를 입력하면 됩니다. 아주 간단하죠? 불러들인 웹 사이트의 내용은 html이라는 객체에 저장하겠습니다. 다음과 같이 입력하세요.

	>>> html = re.urlopen(url)
	# url 객체에 저장된 URL 주소에 해당하는 웹 사이트를 불러옵니다.
	>>> html
	<뭐라고 뭐라고>
	
html에 어떤 내용이 들어 있는지 궁금하다면 read()로 간단히 살펴보면 됩니다. html에는 엄청나게 많은 정보가 들어 있어서 이것을 다 보기는 조금 그렇고, 100개까지만 슬라이싱을 해서 정보를 한번 읽어 보겠습니다.

	>>> html.read()[:100]
	
	
	# html에 저장한 웹 사이트 자료가 일부 출력됩니다.
	
2. 뷰티풀스프로 자료형 변환하기
이제 뷰티풀수프를 사용해 html 객체에 저장한 자료를 정보를 쉽게 추출할 수 있는 형태, 즉 파싱(pasing)이란 쉬운 형태로 변환학겠습니다. 사용법은 다음과 같습니다.
% 파싱(pasing)이란 웹 문서에서 원하는 패턴이나 순서로 자료를 추출해 가공하는 것을 말합니다.

뷰티풀수프 사용법
	bs(html.read(), 'html.paser')
	
다음과 같이 코드를 입력해 html 객체에 뷰티풀수프를 사용한 다음에 soup라는 객체로 저장하겠습니다.

	>>> html = ur.urlopen(url)		# 다시 웹 사이트 내용을 불러와 html에 저장합니다.
	>>> soup = bs(html.read(), 'html.parser')
	
자, 재대로 추출된 soup는 어떤 모양을 하고 있는지 간단히 살펴볼까요? 앞의 과정을 다 정확하게 밟았다면 그냥 soup라고 입력하면 다음과 같은 노란 박스가 나옵니다. 234 줄이나 나와서 스크롤의 압박이 있을 것 같아서 IDLE에서 노란 박스로 숨겨 놓은 것 같습니다.

상자를 더블 클릭하면 앞에서 슬라이싱으로 확인했던 html 객체의 내용과 출력되는 모양이 다른 것을 확인할 수 있습니다.

	>>> soup
	# 노란 상자를 클릭하면 soup 객체의 내용을 확인할 수 있습니다.
	<뭐라고 뭐라고>
	
	
html과 soup의 자료형도 비교해 볼까요? soup의 자료형이 bs4.BeautifulSoup로 만들어진 것을 확인할 수 있습니다.

	>>> type(html)
	<class 'http.client.HTTPResponse'>
	>>> type(soup)
	<class 'bs4.BeautifulSoup>
	
3. 한 줄로 모든 명령을 실행하는 마법의 명령어 만들기
이렇게 해서 원하는 웹 사이트의 자료를 불러왔다면 soup 안에 있는 정보들을 가공할 수 있습니다. 하지만 앞의 과정들이 조금은 귀찮고 불편하죠? URL을 불러내고, html로 저장하고, 그 것을 다시 뷰티풀수프를 사용해서 사용해서 soup에 자장하는 작업까지 해야만 다음 작업을 할 수 있습니다.

파이썬에서는 이 모든 과정을 다음과 같이 한 줄로 표현할 수 있습니다.

	soup = bs(ur.urlopen(URL 주소).read(), 'html.parser')

이 장에서는 앞으로 편하게 이 한 줄의 코드를 '마법의 명령어'라고 부르겠습니다. 이 코드를 저장해 놓으면 나중에 URL 주소만 바꿔서 soup 객체로 바로 불러올 수 있기 때문에 상당히 유용합니다. 제가 개인적으로 여러 번의 시행착오를 가져서 정리한 내용이죠. 파이썬이 익숙해지면 사실 여러 객체를 만들기보다 한 줄 안에 필요한 명령어를 모두 엏는 데 익숙해지게 됩니다. 지금까지 실습환 내용을 마법의 명령어로 정리하면 다음과 같이 단 한 줄이면 됩니다.

	>>> soup = bs(ur.urlopen('http://quotes.toscrape.com/').read(), 'html.parser')
	
Do it 특정 태그에서 텍스트만 추출하기
현재 페이지에 있는 텍스트만 아주 간단하게 모을 수 있는 작업을 해 보겠습니다. 이 작업을 이해하기 위해서는 먼저 HTML의 구조에 대한 간단한 이해가 있어야 합니다.

1.HTML의 구조 살펴보기

HTML을 잘 모르는 분을 위해 구조를 아주 간단히 살펴보겠습니다. HTML은 태그로 둘러싸여 있습니다. 젅체는 <HTML>과 </HTML>로 둘러싸여 있고, 그 안에 머리(head)와 몸(body)이 있습니다. 머리는 <head>와 </head>로, 몸은 <body>와 </body>로 둘러싸여 있습니다. 그리고 제목은 <title>과 </title>, 글 제목은 <h1>와 </h1>, 분몬은 <p>와 </p>등으로 둘러싸여 있습니다.

<HTML>
<head>
	<title> 페이지 제목 </title>
</head>
<body>
	<h1> 글 제목 </h1>
	<p> 글 본문</p>
</body>
</HTML>

이제 이 구조를 참고해 웹 문서 자료에서 원하는 요소를 찾아보겠습니다.

2. find_all 로 원하는 태그만 모으기
find_all 메서드는 특정 태그로 둘러싸인 요소를 찾아내서 리스트 형태로 반환합니다. find_all의 사용법은 간단합니다. 이미 웹 문서 자료를 파싱하기 좋은 형태로 변환해 soup에 저장했으니 soup에 find_all을 입력한 후 괄호 안에 '찾아낼 태그'만 입력하면 됩니다.

soup.find_all(찾아낼 태그)

find_all을 활용하는 방법은 이 장에서 소개할 크롤링 기법의 거의 전부라고 말할 수 있을 정도로 강력한 녀석입니다. find_all의 힘을 알아보기 위해 텍스트만 추출하는 간단한 실습을 해 보겠습니다.

먼저 soup의 내용을 다시 한번 열어 볼까요? 자세히 보시면 <head>, </head>가 둘러싸인 부분, 그리고 <body>가 시작되는 부분을 확인할 수 있습니다. 그리고 <tile>과 </title>로 웹사이트 제목을 둘러싸고 있네요.

	>>> soup
	
	<!DOCTYPE html>
	
	
	
자, 이제 텍스트만 추출하기 위해서 soup 객체를 조금만 더 살펴보겠습니다. 잘 찾아보면 text만 추출할 수 있는 방법이 있을 것 같습니다.

	
	

자세히 보면 가장 첫 번째 나왔던 아인슈타인의 명언 'The world as we have create it is a process of our thinking. It cannot be changed without changing our thinking.'이 <span>과 </span>으로 둘러싸여 있는 것을 확인할 수 있습니다. 이렇게 <span> 태그는 HTML의 특정한 위치에 특정한 서식을 가지는 텍스트 등을 삽입하기 위해 사용합니다. <span>이 사용된 방식을 보면 다음과 같이 text라는 class를 가지고 있는 것을 확인할 수 있습니다.

